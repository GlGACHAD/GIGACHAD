# -*- coding: utf-8 -*-
"""NSFW Disabled: NOP & WAS's Stable Diffusion Colab v0.31 (1.4 Weights)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17

# NSFW Disabled: NOP & WAS's Stable Diffusion Colab v0.31 (1.4 Weights)

WAS will be developing this colab with me, he has been doing great work! The logical thing would be to include him here, throw him some kudos for the great work done when you see him! :)

Trying to make this a one-stop shop for various programs + a GOTO guide on how to install everything locally. If you have suggestions, bug reports, or implementations, feel free to contact me on Discord and/or leave a comment via colab's comment feature. NOP#1337

Stable Diffusion github introduced watermarks and a nsfw filter. Pulling an older version but have not tested yet. Feel free to try out txt2img and let me know if it works or not


Changelog:
- v0.1: Colab creation
- v0.11: Google Drive option for TXT2IMG and some error corrections
- v0.12: Added more options to TXT2IMG
- v0.13: Diffusers added a feature which broke the pipeline with the current implementation, reverted back to an older version
- v0.14: Added in full precision in the diffuser method
- v0.15: Added in file saving in drive for diffusers
- v0.16: Added in prompt saving
- v0.17: Added in the new weights and disabled the NSFW check
- v0.18: Minor adjustments and more details saved in prompt saving
- v0.19: Added in modifier experiments in Diffusers + example. More options to experiment will come with future updates
- v0.20: Low VRAM patch is fixed. Getting 10 it/s with it on with a V100
- v0.21: Diffusers now has an upscaler (Real-ESRGAN) <-- just updated to GFPGAN
- v0.22: Added in a small little fun randomizer
- v0.23: Now I support both upscalers. GFP is really good at faces but kind of sucks at upscaling. If you want the best of two worlds choose "Both" as an upscaler. T4 may have problems with one or both of them, looking at a fix for that (May get lucky with Real-ESRGAN).
- v0.30: A complete code overhaul by WAS#0263 and a bunch of stuff added. With an overhaul, there could also different bugs. Shoot me ( NOP#1337 ) a discord message when you find one and tell WAS that he is awesome when you see him! If there are major bugs, I'll fix them as soon as I can
- v0.31: Forgot to mention last update: No more huggingface login, that's all built-in now. Also, we have new facial enhancement. --> CodeFormer. It's like GFP but not as strong + with a nifty slider

TODO:
- Saving weights in Google Drive
- Option to delete original images if upscaled
- Option to pick older models
- Add in the optimized Stable Diffusion fork ( https://github.com/basujindal/stable-diffusion )
- Implement https://github.com/DamascusGit/stable-diffusion/blob/main/scripts/txt2img_k.py
- Add offline install instructions (50% done)
- Add in k-diffusion
- Add in inpainting
- Add init functionality
- Add in a plms, ddim, etc choice (50% done)
- Option to load a config file to load in preset settings
- More functionality

By NOP#1337 & WAS#0263

Credits:
- WAS#0263 for giving great advice, coding tips, code, and recommendations. A MASSIVE help overhauling this thing
- ùìëùìµùì™ùì∑ùì¨ùììùìÆùìûùìØùìØùì≤ùì¨ùì≤ùì™ùìµ#2485 for inspiring me to put an upscaler in the colab and for bug hunting
- Gecktendo#8043 for helping with the default prompt
- Original TXT2IMG Notebook: Lucas Ferreira da Silva, Madams, Greg Turk


FAQ:

Q: What is the difference between Diffusers and TXT2IMG?

A: Diffusers is the Huggingface Python Library and TXT2IMG is from the Stability AI Github. They both do the same thing, but differently. Whichever you want to use is just personal preference.

Q: Which one should I use?

A: Really just personal preference. For me: Right now I am heavily concentrating on diffusers just because it's a tad easier to work with.

GPU Info
"""

!nvidia-smi

"""# Diffusers Method

We don't need a huggingface login anymore :) just go ahead and click setup
"""

#@title Diffusers Setup

import os, torch, gc

from getpass import getpass



#@markdown ---
#@markdown `LOGIN_GDRIVE`: Click this if you want to log in now instead of later if you plan on using Google Drive<br>
LOGIN_GDRIVE = True #@param {type:"boolean"}
#@markdown ---

def fetch_bytes(url_or_path):
    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):
        from urllib.request import urlopen 
        return urlopen(url_or_path) 
    return open(url_or_path, 'r')


try:
  with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:
    key = f.read().decode('utf-8').split(':')
except OSError as e:
  print(e)
  
huggingface_username = key[0].strip()
huggingface_token = key[1].strip()

def clean_env():
  gc.collect()
  torch.cuda.empty_cache()

clean_env()

#@markdown `LOW_VRAM_PATCH`: Click this if you have CUDA out of memory errors with low settings. If you check this you may be tied to using this setting until the next session restart since it patches various files. <br> This should also speed up iterations but could output lower quality content<br>
#@markdown `ENABLE_NSFW_FILTER`: Will ENABLE the NSFW filter. If you want uncensored results, do not click that. Might need a session restart if you have run this setup prior without it checked since it patches files.<br>
LOW_VRAM_PATCH = False #@param {type:"boolean"}
ENABLE_NSFW_FILTER = False #@param {type:"boolean"}
CLEAR_SETUP_LOG = True #@param{type: 'boolean'}
#markdown Clear the setup log after installation compeltes.

# Enable third-party widgets
from google.colab import output
output.enable_custom_widget_manager()

if LOGIN_GDRIVE and not os.path.exists('/content/drive'):
  from google.colab import drive
  drive.mount('/content/drive')

try: 
  !git lfs install
  !GIT_LFS_SKIP_SMUDGE=0
  # This will take a while
  !git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4
  !pip install -U git+https://github.com/huggingface/diffusers.git
  # Disable NSFW check
  if not ENABLE_NSFW_FILTER:
    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:
      file.write('''
import numpy as np
import torch
import torch.nn as nn

from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel

from ...utils import logging


logger = logging.get_logger(__name__)


def cosine_distance(image_embeds, text_embeds):
    normalized_image_embeds = nn.functional.normalize(image_embeds)
    normalized_text_embeds = nn.functional.normalize(text_embeds)
    return torch.mm(normalized_image_embeds, normalized_text_embeds.T)


class StableDiffusionSafetyChecker(PreTrainedModel):
    config_class = CLIPConfig

    def __init__(self, config: CLIPConfig):
        super().__init__(config)

        self.vision_model = CLIPVisionModel(config.vision_config)
        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)

        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)
        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)

        self.register_buffer("concept_embeds_weights", torch.ones(17))
        self.register_buffer("special_care_embeds_weights", torch.ones(3))

    @torch.no_grad()
    def forward(self, clip_input, images):
        pooled_output = self.vision_model(clip_input)[1]  # pooled_output
        image_embeds = self.visual_projection(pooled_output)

        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().numpy()
        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().numpy()

        result = []
        batch_size = image_embeds.shape[0]
        for i in range(batch_size):
            result_img = {"special_scores": {}, "special_care": [], "concept_scores": {}, "bad_concepts": []}

            # increase this value to create a stronger `nfsw` filter
            # at the cost of increasing the possibility of filtering benign images
            adjustment = 0.0

            for concet_idx in range(len(special_cos_dist[0])):
                concept_cos = special_cos_dist[i][concet_idx]
                concept_threshold = self.special_care_embeds_weights[concet_idx].item()
                result_img["special_scores"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)
                if result_img["special_scores"][concet_idx] > 0:
                    result_img["special_care"].append({concet_idx, result_img["special_scores"][concet_idx]})
                    adjustment = 0.01

            for concet_idx in range(len(cos_dist[0])):
                concept_cos = cos_dist[i][concet_idx]
                concept_threshold = self.concept_embeds_weights[concet_idx].item()
                result_img["concept_scores"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)
                if result_img["concept_scores"][concet_idx] > 0:
                    result_img["bad_concepts"].append(concet_idx)

            result.append(result_img)

        has_nsfw_concepts = [len(res["bad_concepts"]) > 0 for res in result]

        #for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):
        #    if has_nsfw_concept:
        #        images[idx] = np.zeros(images[idx].shape)  # black image

        if any(has_nsfw_concepts):
            logger.warning(
                "Potential NSFW content was detected in one or more images, but the NSFW filter is off."
            )

        return images, has_nsfw_concepts''')
  !pip install transformers
  # make sure you're logged in with `huggingface-cli login`
  from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler

  # lms = LMSDiscreteScheduler(
  #     beta_start=0.00085, 
  #     beta_end=0.012, 
  #     beta_schedule="scaled_linear"
  # )

  model_id = "CompVis/stable-diffusion-v1-4"

  !mkdir diffusers_output
  !pip install pytorch-pretrained-bert
  !pip install spacy ftfy
  !python -m spacy download en
  !pip install scipy
  !git clone --recursive https://github.com/crowsonkb/k-diffusion.git
  !echo $huggingface_token | huggingface-cli login
  if LOW_VRAM_PATCH:
    patched_file = open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py').read().replace('input, self.num_groups, self.weight, self.bias, self.eps)','input, self.num_groups, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)')
    with open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py','w') as file:
      file.write(patched_file)
    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py','w') as file:
      file.write(
    '''
import inspect
import warnings
from typing import List, Optional, Union

import torch

from tqdm.auto import tqdm
from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer

from ...models import AutoencoderKL, UNet2DConditionModel
from ...pipeline_utils import DiffusionPipeline
from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler
from .safety_checker import StableDiffusionSafetyChecker


class StableDiffusionPipeline(DiffusionPipeline):
  def __init__(
      self,
      vae: AutoencoderKL,
      text_encoder: CLIPTextModel,
      tokenizer: CLIPTokenizer,
      unet: UNet2DConditionModel,
      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],
      safety_checker: StableDiffusionSafetyChecker,
      feature_extractor: CLIPFeatureExtractor,
  ):
      super().__init__()
      scheduler = scheduler.set_format("pt")
      self.register_modules(
          vae=vae,
          text_encoder=text_encoder,
          tokenizer=tokenizer,
          unet=unet,
          scheduler=scheduler,
          safety_checker=safety_checker,
          feature_extractor=feature_extractor,
      )

  @torch.no_grad()
  def __call__(
      self,
      prompt: Union[str, List[str]],
      height: Optional[int] = 512,
      width: Optional[int] = 512,
      num_inference_steps: Optional[int] = 50,
      guidance_scale: Optional[float] = 7.5,
      eta: Optional[float] = 0.0,
      generator: Optional[torch.Generator] = None,
      output_type: Optional[str] = "pil",
      **kwargs,
  ):
      if "torch_device" in kwargs:
          device = kwargs.pop("torch_device")
          warnings.warn(
              "`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0."
              " Consider using `pipe.to(torch_device)` instead."
          )

          # Set device as before (to be removed in 0.3.0)
          if device is None:
              device = "cuda" if torch.cuda.is_available() else "cpu"
          self.to(device)

      if isinstance(prompt, str):
          batch_size = 1
      elif isinstance(prompt, list):
          batch_size = len(prompt)
      else:
          raise ValueError(f"`prompt` has to be of type `str` or `list` but is {type(prompt)}")

      if height % 8 != 0 or width % 8 != 0:
          raise ValueError(f"`height` and `width` have to be divisible by 8 but are {height} and {width}.")

      # get prompt text embeddings
      text_input = self.tokenizer(
          prompt,
          padding="max_length",
          max_length=self.tokenizer.model_max_length,
          truncation=True,
          return_tensors="pt",
      )
      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]

      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
      # corresponds to doing no classifier free guidance.
      do_classifier_free_guidance = guidance_scale > 1.0
      # get unconditional embeddings for classifier free guidance
      if do_classifier_free_guidance:
          max_length = text_input.input_ids.shape[-1]
          uncond_input = self.tokenizer(
              [""] * batch_size, padding="max_length", max_length=max_length, return_tensors="pt"
          )
          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]

          # For classifier free guidance, we need to do two forward passes.
          # Here we concatenate the unconditional and text embeddings into a single batch
          # to avoid doing two forward passes
          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

      # get the intial random noise
      latents = torch.randn(
          (batch_size, self.unet.in_channels, height // 8, width // 8),
          generator=generator,
          device=self.device,
      )
      latents = latents.half()

      # set timesteps
      accepts_offset = "offset" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())
      extra_set_kwargs = {}
      if accepts_offset:
          extra_set_kwargs["offset"] = 1

      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)

      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas
      if isinstance(self.scheduler, LMSDiscreteScheduler):
          latents = latents * self.scheduler.sigmas[0]

      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
      # eta (Œ∑) is only used with the DDIMScheduler, it will be ignored for other schedulers.
      # eta corresponds to Œ∑ in DDIM paper: https://arxiv.org/abs/2010.02502
      # and should be between [0, 1]
      accepts_eta = "eta" in set(inspect.signature(self.scheduler.step).parameters.keys())
      extra_step_kwargs = {}
      if accepts_eta:
          extra_step_kwargs["eta"] = eta

      for i, t in tqdm(enumerate(self.scheduler.timesteps)):
          # expand the latents if we are doing classifier free guidance
          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
          if isinstance(self.scheduler, LMSDiscreteScheduler):
              sigma = self.scheduler.sigmas[i]
              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)

          # predict the noise residual
          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)["sample"]

          # perform guidance
          if do_classifier_free_guidance:
              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

          # compute the previous noisy sample x_t -> x_t-1
          if isinstance(self.scheduler, LMSDiscreteScheduler):
              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)["prev_sample"]
          else:
              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)["prev_sample"]

      # scale and decode the image latents with vae
      latents = 1 / 0.18215 * latents
      image = self.vae.decode(latents)

      image = (image / 2 + 0.5).clamp(0, 1)
      image = image.cpu().permute(0, 2, 3, 1).numpy()

      # run safety checker
      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors="pt").to(self.device)
      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)

      if output_type == "pil":
          image = self.numpy_to_pil(image)

      return {"sample": image, "nsfw_content_detected": has_nsfw_concept}
    '''
      )

    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py', 'w') as file:
      file.write(
    '''
import inspect
import warnings
from typing import List, Optional, Union

import torch

from tqdm.auto import tqdm
from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer

from ...models import AutoencoderKL, UNet2DConditionModel
from ...pipeline_utils import DiffusionPipeline
from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler
from .safety_checker import StableDiffusionSafetyChecker


class StableDiffusionPipeline(DiffusionPipeline):
  def __init__(
      self,
      vae: AutoencoderKL,
      text_encoder: CLIPTextModel,
      tokenizer: CLIPTokenizer,
      unet: UNet2DConditionModel,
      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],
      safety_checker: StableDiffusionSafetyChecker,
      feature_extractor: CLIPFeatureExtractor,
  ):
      super().__init__()
      scheduler = scheduler.set_format("pt")
      self.register_modules(
          vae=vae,
          text_encoder=text_encoder,
          tokenizer=tokenizer,
          unet=unet,
          scheduler=scheduler,
          safety_checker=safety_checker,
          feature_extractor=feature_extractor,
      )

  @torch.no_grad()
  def __call__(
      self,
      prompt: Union[str, List[str]],
      height: Optional[int] = 512,
      width: Optional[int] = 512,
      num_inference_steps: Optional[int] = 50,
      guidance_scale: Optional[float] = 7.5,
      eta: Optional[float] = 0.0,
      generator: Optional[torch.Generator] = None,
      output_type: Optional[str] = "pil",
      **kwargs,
  ):
      if "torch_device" in kwargs:
          device = kwargs.pop("torch_device")
          warnings.warn(
              "`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0."
              " Consider using `pipe.to(torch_device)` instead."
          )

          # Set device as before (to be removed in 0.3.0)
          if device is None:
              device = "cuda" if torch.cuda.is_available() else "cpu"
          self.to(device)

      if isinstance(prompt, str):
          batch_size = 1
      elif isinstance(prompt, list):
          batch_size = len(prompt)
      else:
          raise ValueError(f"`prompt` has to be of type `str` or `list` but is {type(prompt)}")

      if height % 8 != 0 or width % 8 != 0:
          raise ValueError(f"`height` and `width` have to be divisible by 8 but are {height} and {width}.")

      # get prompt text embeddings
      text_input = self.tokenizer(
          prompt,
          padding="max_length",
          max_length=self.tokenizer.model_max_length,
          truncation=True,
          return_tensors="pt",
      )
      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]

      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
      # corresponds to doing no classifier free guidance.
      do_classifier_free_guidance = guidance_scale > 1.0
      # get unconditional embeddings for classifier free guidance
      if do_classifier_free_guidance:
          max_length = text_input.input_ids.shape[-1]
          uncond_input = self.tokenizer(
              [""] * batch_size, padding="max_length", max_length=max_length, return_tensors="pt"
          )
          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]

          # For classifier free guidance, we need to do two forward passes.
          # Here we concatenate the unconditional and text embeddings into a single batch
          # to avoid doing two forward passes
          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

      # get the intial random noise
      latents = torch.randn(
          (batch_size, self.unet.in_channels, height // 8, width // 8),
          generator=generator,
          device=self.device,
      )
      latents = latents.half()

      # set timesteps
      accepts_offset = "offset" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())
      extra_set_kwargs = {}
      if accepts_offset:
          extra_set_kwargs["offset"] = 1

      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)

      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas
      if isinstance(self.scheduler, LMSDiscreteScheduler):
          latents = latents * self.scheduler.sigmas[0]

      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
      # eta (Œ∑) is only used with the DDIMScheduler, it will be ignored for other schedulers.
      # eta corresponds to Œ∑ in DDIM paper: https://arxiv.org/abs/2010.02502
      # and should be between [0, 1]
      accepts_eta = "eta" in set(inspect.signature(self.scheduler.step).parameters.keys())
      extra_step_kwargs = {}
      if accepts_eta:
          extra_step_kwargs["eta"] = eta

      for i, t in tqdm(enumerate(self.scheduler.timesteps)):
          # expand the latents if we are doing classifier free guidance
          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
          if isinstance(self.scheduler, LMSDiscreteScheduler):
              sigma = self.scheduler.sigmas[i]
              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)

          # predict the noise residual
          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)["sample"]

          # perform guidance
          if do_classifier_free_guidance:
              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

          # compute the previous noisy sample x_t -> x_t-1
          if isinstance(self.scheduler, LMSDiscreteScheduler):
              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)["prev_sample"]
          else:
              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)["prev_sample"]

      # scale and decode the image latents with vae
      latents = 1 / 0.18215 * latents
      image = self.vae.decode(latents)

      image = (image / 2 + 0.5).clamp(0, 1)
      image = image.cpu().permute(0, 2, 3, 1).numpy()

      # run safety checker
      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors="pt").to(self.device)
      safety_cheker_input.pixel_values = safety_cheker_input.pixel_values.half()
      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)

      if output_type == "pil":
          image = self.numpy_to_pil(image)

      return {"sample": image, "nsfw_content_detected": has_nsfw_concept}
    ''')
    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_auth_token=True).to("cuda")
    del pipe.vae.encoder
  else:
    pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to("cuda")
except OSError as e:
  raise e
except BaseException as e:
  raise e
finally:
  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()
  print("Setup complete.")

from PIL import Image
import random, time, shutil, sys
from contextlib import contextmanager, nullcontext
from torch import autocast
from IPython.display import clear_output
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
#@title Render Images
clean_env()
PROMPT = "matte potrait of a young cyberpunk woman as a Disney Princess, full-frame, complex picture, intricate, fine details, vogue, trending on artstation, artgerm, greg manchess, studio ghibli, Disney, Star Wars" #@param {type:'string'}
PROMPT_FILE = '' #@param {type: 'string'}
#@markdown `PROMPT_FILE` is a text file that contains a prompt per line. 
STEPS = 160 #@param {type:"slider", min:5, max:500, step:5} 
#@markdown Diffusion steps determine the quality of the final image
SEED = 0 #@param {type:'integer'}
#@markdown The seed used for the generation. Leave at `0` for random.
NUM_ITERS = 6 #@param {type:"slider", min:1, max:100, step:1} 
#@markdown Number of iterations for a given prompt.
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.8 #@param {type:"slider", min:0, max:25, step:0.1}
#@markdown The CFG scale determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.<br>
PRECISION = "autocast" #@param ["full","autocast"]
#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>
SAVE_PROMPT_DETAILS = True #@param {type:"boolean"}
#@markdown ---
USE_DRIVE_FOR_PICS = True #@param {type:"boolean"}
DRIVE_PIC_DIR = "AI_PICS" #@param {type:"string"}
#@markdown ---
#@markdown `IMAGE_UPSCALER`: May not work with the Tesla T4. GFP is pretty good at faces and the enhanced Real-ESRGAN is a pretty good uspcaler. If both is selected, then GFPGAN will act as a face enhancer and Real-ESRGAN will act as the upscaler. Same applies with Codeformer, which seems to be a little more mild than GFP.<br>Recommendations: GFPGAN if you only have faces or People. ESRGAN if you have no people in your prompt. Both if you have people and other things in your prompt<br>Note: ESRGAN only support 2x, 4x, and 8x, if any other value is selected, it will pick the nearest value<br>
IMAGE_UPSCALER = "CodeFormer + Enhanced ESRGAN" #@param ["None","GFPGAN","Enhanced Real-ESRGAN", "GFPGAN + Enhanced ESRGAN", "CodeFormer", "CodeFormer + Enhanced ESRGAN"]
UPSCALE_AMOUNT = 2 #@param {type:"raw"}
#@markdown `CODEFORMER_FIDELITY`: Only applies if the upscaler includes Codeformer. Balance the quality (lower number) and fidelity (higher number)<br>
CODEFORMER_FIDELITY = 0.5 #@param {type:"slider", min:0, max:1, step:0.01}
precision_scope = autocast if PRECISION=="autocast" else nullcontext
ORIG_SEED = SEED

# %cd /content/
OUTDIR = '/content/diffusers_output'

if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):
  from google.colab import drive
  drive.mount('/content/drive')
if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):
  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR
if USE_DRIVE_FOR_PICS:
  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'

if "GFPGAN" in IMAGE_UPSCALER:
  if not os.path.exists('/content/GFPGAN'):
    !git clone https://github.com/TencentARC/GFPGAN.git
#     %cd GFPGAN
    # Set up the environment
    # Install basicsr - https://github.com/xinntao/BasicSR
    # We use BasicSR for both training and inference
    !pip install basicsr
    # Install facexlib - https://github.com/xinntao/facexlib
    # We use face detection and face restoration helper in the facexlib package
    !pip install facexlib
    # Install other depencencies
    !pip install -r requirements.txt
    !python setup.py develop
    !pip install realesrgan  # used for enhancing the background (non-face) regions
    # Download the pre-trained model
    # !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models
    # Now we use the V1.3 model for the demo
    !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models
#     %cd /content/
    clear_output()
    
if "ESRGAN"in IMAGE_UPSCALER:
  def upscale(image):
    sr_image = model.predict(np.array(image))
    return sr_image
  if not os.path.exists('/content/Real-ESRGAN'):
    !git clone https://github.com/sberbank-ai/Real-ESRGAN
    !pip install -r Real-ESRGAN/requirements.txt
    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O Real-ESRGAN/weights/RealESRGAN_x2.pth
    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O Real-ESRGAN/weights/RealESRGAN_x4.pth
    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O Real-ESRGAN/weights/RealESRGAN_x8.pth
#     %cd Real-ESRGAN
    from realesrgan import RealESRGAN
    clear_output()

    device = torch.device('cuda')

    if not os.path.exists(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):
      def closest_value(input_list, input_value):
        difference = lambda input_list : abs(input_list - input_value)
        res = min(input_list, key=difference)
        return res
      nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)
      print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')
      UPSCALE_AMOUNT = nearest_value

    model = RealESRGAN(device, scale = UPSCALE_AMOUNT)
    model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')
#     %cd ..

if "CodeFormer" in IMAGE_UPSCALER:
  if not os.path.exists('/content/CodeFormer'):
#     %cd /content
    !git clone https://github.com/sczhou/CodeFormer.git
#     %cd CodeFormer
    !pip install -r requirements.txt
    # Install basicsr
    !python basicsr/setup.py develop

    # Download the pre-trained model
    !python scripts/download_pretrained_models.py facelib
    !python scripts/download_pretrained_models.py CodeFormer
    !mkdir temp
    !mkdir results
#     %cd /content/


# Diffuse Function
def diffuse_run():

    global SEED

    if ORIG_SEED == 0:
      if iteration is 0:
        SEED = random.randint(0,sys.maxsize)
      if iteration is not 0:
        SEED += 1
    else:
      if iteration > 0:
        SEED += 1
    gen_seed = torch.Generator("cuda").manual_seed(SEED)
    epoch_time = int(time.time())
    print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')
    clean_env()
    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)["sample"][0]
    display(image)
    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'
    filedir = f'{OUTDIR}/{filename}'
    image.save(filedir)
    clean_env()
    if IMAGE_UPSCALER == "GFPGAN":
      print('Face Enhancing and Upscaling... ')
#       %cd GFPGAN
      !python inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s $UPSCALE_AMOUNT --bg_upsampler realesrgan
      display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))
#       %cd ..
      print(f'Moving enhanced image to {OUTDIR}')
      shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
      clean_env()
    if IMAGE_UPSCALER == "Enhanced Real-ESRGAN":
      print('Upscaling... ')
      sr_image = upscale(image)
      display(sr_image)
      try:
        sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')
      except NameError:
        sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')
      clean_env()
    if IMAGE_UPSCALER == "GFPGAN + Enhanced ESRGAN":
      print('Face Enhancing... ')
#       %cd GFPGAN
      !python inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s 1 --bg_upsampler realesrgan
      display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))
#       %cd ..
      shutil.copy(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
      clean_env()
      print('Upscaling... ')
      sr_image = upscale(Image.open(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'))
      display(sr_image)
      try:
        sr_image.save(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
      except NameError:
        sr_image.save(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
      clean_env()
    if IMAGE_UPSCALER == "CodeFormer":
      print("Face enhancing and Upscaling... ")
      # It was behaving weird, hence why I am doing this the weird way
      !cp $filedir /content/CodeFormer/temp/
#       %cd /content/CodeFormer/
      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale $UPSCALE_AMOUNT --bg_upsampler realesrgan
      os.remove(f'/content/CodeFormer/temp/{filename}')
      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
#       %cd /content
      display(Image.open(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'))
      clean_env()
    if IMAGE_UPSCALER == "CodeFormer + Enhanced ESRGAN":
      print("Face enhancing... ")
      !cp $filedir /content/CodeFormer/temp/
#       %cd /content/CodeFormer/
      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale 1 --bg_upsampler realesrgan
      os.remove(f'/content/CodeFormer/temp/{filename}')
      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
#       %cd /content
      display(Image.open(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'))
      clean_env()
      print("Upscaling... ")
      sr_image = upscale(Image.open(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'))
      display(sr_image)
      sr_image.save(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
      clean_env()
# End Diffuse Function

epoch_time = int(time.time())
if SAVE_PROMPT_DETAILS:
  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:
        file.write(f'{PROMPT}\n\nHeight: {HEIGHT}\nWidth: {WIDTH}\nSeed: {SEED}\nScale: {SCALE}\nPrecision: {PRECISION}\n')

PROMPTS = []
if PROMPT_FILE not in ['','none']:
    try:
        with open(PROMPT_FILE, "r") as f:
            PROMPTS = f.read().splitlines()
    except OSError as e:
        raise e

if PROMPT not in ['', 'none']:
    PROMPTS.insert(0, PROMPT)

with precision_scope("cuda"):

    for pi in PROMPTS:
        PROMPT = pi
        for iteration in range(NUM_ITERS):
            try:
                diffuse_run()
            except RuntimeError as e:
                if 'out of memory' in str(e):
                    print(f"\u001b[31m\u001b[1m\u001b[4mCRITICAL ERROR\u001b[0m: {gpu_name} ran out of memory! Flushing memory to save session...")
                    pass
                else:
                    raise e
            except KeyboardInterrupt as e:
              print('\nSeed used for this exited run:', SEED)
              raise SystemExit('\33[33mExecution interrupted by user.\33[0m')
            finally:
              print("Cleaning up...\n")
              clean_env()

"""## Diffuser Experiments (run through the Diffuser setup first)"""

#@title Modifier Tester
#@markdown `MODIFIER_FILE`: location of a text file with a list of modifiers seperated by newline.<br>You will need to upload it. Then right click on the file in colab and then click "copy path" (If you can't find it, click on the folder icon on the left pane). Then paste it in the box. See modifier_examples.txt for an example (if you're lazy, just edit the file. It will populate AFTER the first run)<br>
#@markdown `BASE_PROMPT`: the prompt against which the modifiers will be tested<br>
with open('modifier_examples.txt','w') as file:
  file.write('Canon\nNikon\nPanasonic\nSony\nDigital Painting\nMatte Painting\nDrawing from a 5 year old\nPasta Art\nI made this while on acid')

MODIFIER_FILE = "/content/modifier_examples.txt" #@param {type:'string'}
BASE_PROMPT = "A dog playing in a field" #@param {type:'string'}
STEPS = 50 #@param {type:"slider", min:5, max:500, step:5} 
SEED = 42 #@param {type:'integer'}
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.8 #@param {type:"slider", min:0, max:25, step:0.1}
PRECISION = "autocast" #@param ["full","autocast"]
SEED = torch.Generator("cuda").manual_seed(SEED)
import random
import torch
from contextlib import contextmanager, nullcontext
import time
import os
from torch import autocast

OUTDIR = '/content/experiments'
precision_scope = autocast if PRECISION=="autocast" else nullcontext
with open(MODIFIER_FILE) as file:
  for line in file.readlines():
    line = line.strip()
    PROMPT = f"{BASE_PROMPT}, {line}"
    print(f"Running: {PROMPT}")
    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=SEED)["sample"][0]  
    display(image)
    try:
      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')
    except FileNotFoundError:
      !mkdir $OUTDIR
      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')

#@title Randomizer, aka: I feel lucky/Fuck my shit up
import os
import random
WORDS_AMOUNT = 30 #@param {type:"integer"}
STEPS = 90 #@param {type:"slider", min:5, max:500, step:5}
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.7 #@param {type:"slider", min:0, max:25, step:0.1}
PRECISION = "autocast" #@param ["full","autocast"]
precision_scope = autocast if PRECISION=="autocast" else nullcontext

if not os.path.exists('words.txt'):
  !wget https://gist.githubusercontent.com/h3xx/1976236/raw/bbabb412261386673eff521dddbe1dc815373b1d/wiki-100k.txt -O words.txt
with open('words.txt') as file:
  words = file.readlines()
  prompt = ""
  for iteration in range(WORDS_AMOUNT):
    again = True
    while again:
      word = random.choice(words).strip()
      if not '#' in word:
        again = False
    prompt += f'{random.choice(words).strip()}, '
prompt = prompt[:-2]
print(f'Prompt: {prompt}')
with precision_scope("cuda"):
  seed = torch.Generator("cuda").manual_seed(random.randint(0,4294967295))
  image = pipe(prompt, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)["sample"][0] 
  display(image)

"""#TXT2IMG Method"""

#@title Huggingface Login
from getpass import getpass

huggingface_username = '' #@param {type:"string"}
huggingface_token = '' #@param {type:"string"}

# Commented out IPython magic to ensure Python compatibility.
#@title TXT2IMG Setup
import os
!pip install -q condacolab
import condacolab
import torch
torch.cuda.empty_cache()
condacolab.install()
root_code = root_model = "/content/stableai"
code_dir = root_code + "/stable-diffusion"
import os
if not os.path.isdir(root_code):
  !mkdir $root_code
# %cd $root_code
!git clone https://github.com/DamascusGit/stable-diffusion.git
!mamba env update -n base -f stable-diffusion/environment.yaml
!pip install torchmetrics==0.6.0
!pip install kornia==0.6)
if not os.path.isdir(root_model):
  !mkdir $root_model
# %cd $root_model
!git lfs install
!GIT_LFS_SKIP_SMUDGE=0
# Will take a long time
!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original
import numpy as np
import os
from PIL import Image
def display_last_grid(grid_dir):
  dir_list = os.listdir(grid_dir)
  dir_list.sort()
  #print (dir_list)
  last_image = dir_list[-2]
  img = Image.open(grid_dir + "/" + last_image).convert('RGB')
  target_size = 600
  img.thumbnail((target_size,target_size))
  display (img)
!mkdir /content/txt2img_output
# %cd $code_dir

# Commented out IPython magic to ensure Python compatibility.

import random
import torch
from contextlib import contextmanager, nullcontext
import time
import os
from torch import autocast
torch.cuda.empty_cache()
PROMPT = "matte potrait of a young cyberpunk woman as a Disney Princess, full-frame, complex picture, intricate, fine details, vogue, trending on artstation, artgerm, greg manchess, studio ghibli, Disney, Star Wars" #@param {type:'string'}
STEPS = 145 #@param {type:"slider", min:5, max:500, step:5} 
SEED = 0 #@param {type:'integer'}
NUM_ITERS = 6 #@param {type:"slider", min:1, max:100, step:1} 
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.8 #@param {type:"slider", min:0, max:25, step:0.1}
PRECISION = "autocast" #@param ["full","autocast"]
SAVE_PROMPT_DETAILS = True #@param {type:"boolean"}
USE_DRIVE_FOR_PICS = False #@param {type:"boolean"}
DRIVE_PIC_DIR = "AI_PICS" #@param {type:"string"}
IMAGE_UPSCALER = True #@param {type:"boolean"}
UPSCALE_AMOUNT = "8" #@param ["2","4", "8"]
precision_scope = autocast if PRECISION=="autocast" else nullcontext
ORIG_SEED = SEED

# %cd /content/

if IMAGE_UPSCALER:
  def upscale(image):
    sr_image = model.predict(np.array(image))
    return sr_image
  if not os.path.exists('/content/Real-ESRGAN'):
    !git clone https://github.com/sberbank-ai/Real-ESRGAN
    !pip install -r Real-ESRGAN/requirements.txt
    !gdown https://drive.google.com/uc?id=1pG2S3sYvSaO0V0B8QPOl1RapPHpUGOaV -O Real-ESRGAN/weights/RealESRGAN_x2.pth
    !gdown https://drive.google.com/uc?id=1SGHdZAln4en65_NQeQY9UjchtkEF9f5F -O Real-ESRGAN/weights/RealESRGAN_x4.pth
    !gdown https://drive.google.com/uc?id=1mT9ewx86PSrc43b-ax47l1E2UzR7Ln4j -O Real-ESRGAN/weights/RealESRGAN_x8.pth
#   %cd /content/Real-ESRGAN
  from realesrgan import RealESRGAN
  from PIL import Image
  import numpy as np
  import torch

  device = torch.device('cuda')
  model = RealESRGAN(device, scale = int(UPSCALE_AMOUNT))
  model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')
#   %cd /content/

if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):
  from google.colab import drive
  drive.mount('/content/drive')
if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):
  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR
if USE_DRIVE_FOR_PICS:
  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'
else:
  OUTDIR = '/content/diffusers_output'
epoch_time = int(time.time())
if SAVE_PROMPT_DETAILS:
  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:
        file.write(f'{PROMPT}\n\nHeight: {HEIGHT}\nWidth: {WIDTH}\nSeed: {SEED}\nScale: {SCALE}\nPrecision: {PRECISION}\n')
with precision_scope("cuda"):
  for iteration in range(NUM_ITERS):
    
    if ORIG_SEED == 0:
      rand_num = random.randint(0,4294967295)
      gen_seed = torch.Generator("cuda").manual_seed(rand_num)
    else:
      gen_seed = torch.Generator("cuda").manual_seed(SEED)
    epoch_time = int(time.time())
    try:
      print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')
    except NameError:
      print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')
    
    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)["sample"][0]  
    display(image)
    try:
      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')
    except NameError:
      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')
    print('Upscaling... ')
    sr_image = upscale(image)
    display(sr_image)
    try:
      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')
    except NameError:
      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')
#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>
#@markdown `Out of Memory error`: If the VRAM stacks right after execution, sometimes it helps waiting for a minute before running it again. Looking at ways to force it to clear the VRAM

"""#How to install offline

https://rentry.org/SDInstallation
"""